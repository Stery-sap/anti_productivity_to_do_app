<img width="3188" height="1202" alt="frame (3)" src="https://github.com/user-attachments/assets/517ad8e9-ad22-457d-9538-a9e62d137cd7" />


# Anti-Productive To-Do AppüéØ


## Basic Details
### Team Name: Afsa


### Team Members
- Member 1: Afna V.S. - College of Engineering, Karunagappally
- Member 2: Stery Alex Panicker - College of Engineering, Karunagappally

### Project Description
This anti-productivity app features a sarcastic AI that humorously questions your tasks, aiming to make you genuinely confused about their worth. If sufficiently demotivated by its witty insights, the task is mercifully cancelled, saving you from pointless effort. Embrace sweet unproductivity!

### The Problem (that doesn't exist)
Traditional To-Do Apps: They often create a cycle of guilt and pressure. You add a task, and it sits there, becoming a source of stress until it's done or you mark it as complete. They encourage adding anything and everything, regardless of its true value.

### The Solution (that nobody asked for)
Our solution created a full-stack web app with a static frontend and a Node.js backend. The frontend makes API calls to the backend, which securely uses the Gemini LLM to generate dynamic, sarcastic questions. The app analyzes user responses to these questions to score their motivation. If motivation is too low, the app cancels the task and will not be added on the task list, otherwise, it adds the task. This entire system is designed to be funny and engaging, ultimately helping users identify and avoid pointless tasks.

## Technical Details
### Technologies/Components Used
Software:
- Frontend
    - HTML
    - CSS3
    - JavaScript
- Backend
    - Node.js
    - Express.js
    - dotenv
    - CORS
    - Google Gemini API

### Implementation
For Software:
  The frontend is a static web page built with HTML, CSS, and JavaScript. It displays the UI, including a cycling intro, a to-do list, and a reflective modal.

The backend is a Node.js Express server that acts as a secure intermediary. It receives requests from the frontend and communicates with the Google Gemini LLM API. The backend contains the core logic for constructing LLM prompts, parsing the AI's responses (questions, scores, and cancellation reasons), and securely managing the API key.

When the user interacts with the app, the frontend sends data to the backend. The backend uses this data to interact with the LLM and sends a structured response back to the frontend, which then updates the UI.

# Installation
 Inside the server folder,
     npm init -y
   Install Backend Dependencies:
     npm install express cors dotenv @google/generative-ai

# Run
Backend
 - To install dependencies (run once):
     npm install
 - To start the backend server:
     node server.js     

### Project Documentation
For Software:
1. Project Overview
The Anti-Productivity To-Do App is a unique, full-stack web application designed to challenge traditional productivity mindsets. Its core purpose is to help users identify and avoid "busywork" by engaging them in a humorous, sarcastic, and demotivating reflection process before they add a new task.

Core Problem Solved
The app addresses the psychological problem of mindless productivity and the pressure to add tasks that hold no real value. It acts as a humorous gatekeeper, forcing users to confront their intentions and decide if a task is truly worth the effort.

Key Features
Sarcastic AI Persona: A Large Language Model (LLM) is configured with a cynical, humorous, and demotivating persona to drive the user's reflection.
Dynamic Questioning: Questions asked during the reflection phase are dynamically generated by the LLM, taking into account the user's task and previous answers.
Motivation Scoring: The LLM evaluates each of the user's responses, assigning a "likelihood score" (1-10) to gauge their actual motivation.
Intelligent Task Cancellation: If the user's average score falls below a set threshold, the app cancels the task.
Streamlined Intro: A timed, automated sequence of static, comic messages introduces the app's personality without a traditional loading screen.

2. Technology Stack
The project is built on a modern, decoupled architecture.
Frontend (Client-Side):
HTML5: Provides the application's structure.
CSS3: Handles all styling and layout.
JavaScript (ES6+): Manages all client-side logic, UI interactions, and asynchronous communication.
Backend (Server-Side):
Node.js: The runtime environment for the backend server.
Express.js: A minimal web framework used to create the API endpoints.
dotenv: Securely loads environment variables (e.g., API keys).
cors: A middleware that enables cross-origin requests, allowing the frontend to communicate with the backend.
Google Gemini API (gemini-1.5-flash): The LLM that powers the dynamic reflection feature.


3. Implementation Details
Frontend (index.html, script.js)
The frontend is decoupled from the backend. The script.js file contains a global constant for the backend URL, which is the only hardcoded connection point between the two.
The startAutomaticIntroMessageCycle function uses a recursive setTimeout to handle the timed display of static intro messages.
The fetchAndDisplayNextReflectiveQuestion function is an async function that sends a POST request to the backend's /generate-reflective-question endpoint.
The makeFinalTaskDecision function processes the final LLM response, calculates the average score, and determines whether to add the task or display the LLM's cancellation reason.
Backend (server.js)
The server.js file is an Express server with a single primary endpoint: POST /generate-reflective-question.
This endpoint receives the task and conversationHistory from the frontend.
It constructs a carefully engineered LLM prompt that includes the app's persona and context.
Final Turn Logic: A conditional check (conversationHistory.length >= 5) triggers a special prompt for the 6th turn. This prompt instructs the LLM to provide both a likelihood_score and, conditionally, a cancellation_reason in a structured JSON format.
LLM Integration: The GoogleGenerativeAI client is used to make a secure API call to the gemini-1.5-flash model. The responseMimeType: "application/json" setting is used to instruct the LLM to output valid JSON.
Robustness: try...catch blocks are used for error handling, with a final catch that provides a safe and informative error response to the frontend, preventing the server from crashing. The backend's environment variables are used to securely manage the API key.

4. Deployment and Scalability
The project uses a standard deployment pattern for a full-stack application. The frontend is hosted on a static host (Netlify) and the backend is hosted on a dynamic server (Render). This separation allows for independent scaling and maintenance of each component.
For increased usage beyond free-tier limits, both the LLM API and the Render backend can be upgraded to paid plans without requiring significant changes to the application code.

# Screenshots (Add at least 3)
![Screenshot1]  https://drive.google.com/file/d/1wVJeemZKh03XbmKgSSTFGXIfNOE-Pf4r/view?usp=sharing
entry part

![Screenshot2] https://drive.google.com/file/d/128LvhhdbisXDcsfVyAZ1vxhw2-WdGIKX/view?usp=sharing
asking to add the task

![Screenshot3] https://drive.google.com/file/d/178ul8Z6XNboxtCfT_kGDl1YPB4B6v-Ir/view?usp=sharing
asking confusing questions on the task added







---
Made with ‚ù§Ô∏è at TinkerHub Useless Projects 

![Static Badge](https://img.shields.io/badge/TinkerHub-24?color=%23000000&link=https%3A%2F%2Fwww.tinkerhub.org%2F)
![Static Badge](https://img.shields.io/badge/UselessProjects--25-25?link=https%3A%2F%2Fwww.tinkerhub.org%2Fevents%2FQ2Q1TQKX6Q%2FUseless%2520Projects)



